# GOES Satellite Data Pipeline - Reporte de Reproducibilidad

## üìã Resumen del Proyecto

Este reporte documenta la implementaci√≥n exitosa de un pipeline de datos en tiempo real para procesar datos GOES EXIS/SFXR desde el repositorio del CITIC de la Universidad de Costa Rica. El pipeline implementa un flujo completo: **CITIC ‚Üí Airflow ‚Üí Kafka ‚Üí Druid ‚Üí Superset**.

### üéØ Objetivos Logrados
- ‚úÖ Descarga autom√°tica de archivos NetCDF reales desde CITIC
- ‚úÖ Procesamiento de datos GOES EXIS/SFXR en tiempo real
- ‚úÖ Pipeline de streaming funcional con todas las columnas requeridas
- ‚úÖ Visualizaci√≥n de datos en Superset
- ‚úÖ Documentaci√≥n completa del proceso

---

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    CITIC    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Airflow    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Kafka    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Druid    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Superset   ‚îÇ
‚îÇ  (NetCDF)   ‚îÇ    ‚îÇ (ETL/DAG)    ‚îÇ    ‚îÇ (Streaming) ‚îÇ    ‚îÇ (Storage)   ‚îÇ    ‚îÇ(Visualization)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes del Pipeline
- **Apache Airflow**: Orquestaci√≥n y descarga de datos
- **Apache Kafka**: Streaming de datos en tiempo real
- **Apache Druid**: Almacenamiento anal√≠tico
- **Apache Superset**: Visualizaci√≥n y dashboards
- **PostgreSQL**: Metadatos y configuraci√≥n
- **Docker Compose**: Containerizaci√≥n de servicios

---

## üõ†Ô∏è Proceso de Implementaci√≥n

### 1. Configuraci√≥n del Entorno Base

**Problema inicial**: El proyecto base ten√≠a un pipeline funcional pero trabajaba con datos simulados. Necesit√°bamos integrar datos reales de GOES desde CITIC.

**Decisi√≥n**: Mantener la arquitectura existente y agregar capacidades de descarga real.

### 2. Investigaci√≥n de Fuentes de Datos

**Desaf√≠o**: Determinar c√≥mo acceder a los datos GOES desde CITIC.

**Proceso de descubrimiento**:
```bash
URL del repositorio: https://nube.citic.ucr.ac.cr/index.php/s/3CcdjpMxsiYtagr
Ruta espec√≠fica: /1. GOES/Repositorio01/EXIS/SFXR/20230426/
Token de acceso: 3CcdjpMxsiYtagr
```

**M√©todos probados**:
1. WebDAV con autenticaci√≥n ‚ùå
2. HTTP directo con paths URL-encoded ‚ùå  
3. WebDAV p√∫blico con token como usuario ‚ùå
4. **HTTP directo con URL base DAV** ‚úÖ

**URL que funciona**:
```
https://nube.citic.ucr.ac.cr/public.php/dav/files/3CcdjpMxsiYtagr/1. GOES/Repositorio01/EXIS/SFXR/20230426/archivo.nc
```

### 3. Desarrollo del Script de Pruebas

**Decisi√≥n**: Crear scripts independientes para validar la descarga antes de integrar al DAG.

**Scripts creados**:
- `test_citic_download.py`: Prueba inicial WebDAV
- `test_citic_download_v2.py`: Versi√≥n mejorada con m√∫ltiples m√©todos

**Resultado**: Descarga exitosa de archivo real de 187 KB (NetCDF v√°lido).

### 4. Integraci√≥n al DAG de Airflow

**Modificaciones principales**:

```python
# URL base que funciona
base_dav_url = "https://nube.citic.ucr.ac.cr/public.php/dav/files/3CcdjpMxsiYtagr"

# Archivos espec√≠ficos reales
known_files = [
    "1. GOES/Repositorio01/EXIS/SFXR/20230426/OR_EXIS-L1b-SFXR_G18_s20231160000599_e20231160001294_c20231160001297.nc"
]
```

### 5. Resoluci√≥n de Problemas de Procesamiento

**Problema encontrado**: Error en procesamiento NetCDF
```
Error: cannot access local variable 'datetime' where it is not associated with a value
```

**Soluci√≥n**: Conflicto de nombres entre m√≥dulo datetime y variable local
```python
# Antes (problem√°tico)
from datetime import datetime, timedelta
ref_time = datetime.strptime(...)

# Despu√©s (solucionado)  
from datetime import datetime as dt, timedelta
ref_time = dt.strptime(...)
```

### 6. Configuraci√≥n de Druid

**Datasource configurado**: `druid-goes-satellite-datasource.json`

**Columnas procesadas**:
- `product_time`: Tiempo del producto
- `time`: Timestamp Unix (columna principal)
- `solar_array_current_channel_index_label`: Etiqueta del canal
- `irradiance_xrsa1`, `irradiance_xrsa2`: Irradiancia XRS-A
- `irradiance_xrsb1`, `irradiance_xrsb2`: Irradiancia XRS-B  
- `primary_xrsb`: XRS-B primario
- `dispersion_angle`: √Ångulo de dispersi√≥n
- `integration_time`: Tiempo de integraci√≥n
- `source_file`: Archivo fuente
- `extraction_timestamp`: Timestamp de extracci√≥n
- `file_size_mb`: Tama√±o del archivo

### 7. Configuraci√≥n de Superset

**Conexi√≥n a Druid**:
```
SQLAlchemy URI: druid://router:8888/druid/v2/sql
Display Name: GOES Satellite Data
```

---

## üì¶ Dependencias

### Dependencias del Sistema
```bash
- Docker >= 20.0
- Docker Compose >= 2.0
- Python >= 3.8 (para scripts de prueba)
- 8GB RAM m√≠nimo
- 20GB espacio en disco
```

### Dependencias de Python (Airflow Container)
```txt
# Archivo: app_airflow/requirements.txt
psycopg2>=2.9.10
kafka-python>=2.2.3
pyspark>=3.5.5
numpy>=2.0.2
pandas>=2.2.3
requests>=2.31.0
netCDF4>=1.6.4
sunpy>=5.0.1
webdavclient3>=3.14.6  # Agregado para CITIC
```

### Paquetes Python Locales (para scripts de prueba)
```bash
pip install webdavclient3 requests netCDF4
```

### Versiones de Servicios Docker
```yaml
# docker-compose.yaml
services:
  airflow: # Custom build con dependencias GOES
  kafka: confluentinc/cp-kafka:latest
  druid: apache/druid:33.0.0
  superset: # Custom build
  postgres: postgres:latest
  spark: apache/spark:3.5.1-scala2.12-java11-python3-ubuntu
```

---

## üìñ Manual de Uso

### Paso 1: Preparaci√≥n del Entorno

1. **Clonar el repositorio**:
```bash
git clone <repositorio-url>
cd obdi-opensourcedatapipeline-main
```

2. **Verificar Docker**:
```bash
docker --version
docker-compose --version
```

3. **Instalar dependencias locales** (opcional, para scripts de prueba):
```bash
pip install webdavclient3 requests netCDF4
```

### Paso 2: Configuraci√≥n de Servicios

1. **Iniciar servicios**:
```bash
docker-compose up -d
```

2. **Verificar estado**:
```bash
docker-compose ps
```

3. **Verificar logs** (si hay problemas):
```bash
docker-compose logs airflow
docker-compose logs druid
```

### Paso 3: Configuraci√≥n del Pipeline GOES

1. **Acceder a Airflow**:
   - URL: http://localhost:3000
   - Usuario: admin / admin

2. **Activar el DAG GOES**:
   - Buscar: `goes_satellite_etl`
   - Activar el toggle
   - Ejecutar manualmente: "Trigger DAG"

3. **Monitorear ejecuci√≥n**:
   - Ver logs de tareas individuales
   - Verificar que `download_goes_data` descargue archivo real
   - Confirmar que `extract_goes_satellite_data` procese 30 registros

### Paso 4: Configuraci√≥n de Druid

1. **Acceder a Druid Console**:
   - URL: http://localhost:8888

2. **Configurar datasource**:
```bash
# Copiar configuraci√≥n
cp druid-goes-satellite-datasource.json /ruta/configuracion/
```

3. **Verificar ingesta**:
```sql
SELECT COUNT(*) FROM goes_satellite_datasource
```

### Paso 5: Configuraci√≥n de Superset

1. **Acceder a Superset**:
   - URL: http://localhost:8088
   - Usuario: admin / admin

2. **Agregar conexi√≥n a Druid**:
   - Data ‚Üí Databases ‚Üí +Database
   - Tipo: Apache Druid
   - SQLAlchemy URI: `druid://router:8888/druid/v2/sql`
   - Display Name: "GOES Satellite Data"

3. **Crear dataset**:
   - Data ‚Üí Datasets ‚Üí +Dataset
   - Database: GOES Satellite Data
   - Table: goes_satellite_datasource

4. **Crear visualizaciones**:
   - Charts ‚Üí +Chart
   - Dataset: goes_satellite_datasource
   - Tipos recomendados: Time Series, Scatter Plot, Heat Map

### Paso 6: Verificaci√≥n de Datos Reales

**Consulta de verificaci√≥n**:
```sql
SELECT 
    source_file,
    COUNT(*) as record_count,
    MIN(__time) as first_record,
    MAX(__time) as last_record
FROM goes_satellite_datasource 
GROUP BY source_file
ORDER BY record_count DESC
```

**Resultado esperado**:
```
source_file: OR_EXIS-L1b-SFXR_G18_s20231160000599_e20231160001294_c20231160001297.nc
record_count: 30
```

### Paso 7: An√°lisis de Datos

**Consultas √∫tiles**:

1. **Promedio de irradiancia por hora**:
```sql
SELECT 
    TIME_FLOOR(__time, 'PT1H') AS hour,
    AVG(irradiance_xrsa1) AS avg_xrsa1,
    AVG(irradiance_xrsa2) AS avg_xrsa2
FROM goes_satellite_datasource
GROUP BY 1
ORDER BY 1
```

2. **Valores m√°ximos por d√≠a**:
```sql
SELECT 
    TIME_FLOOR(__time, 'P1D') AS day,
    MAX(irradiance_xrsa1) AS max_xrsa1,
    MAX(primary_xrsb) AS max_xrsb
FROM goes_satellite_datasource
GROUP BY 1
```

---

## üîß Resoluci√≥n de Problemas

### Problema: Docker no inicia
**Soluci√≥n**:
```bash
# Reiniciar Docker Desktop
# Verificar recursos disponibles (8GB RAM m√≠nimo)
docker system prune -a
docker-compose down
docker-compose up -d
```

### Problema: DAG no descarga archivos reales
**Diagn√≥stico**:
```bash
# Verificar logs
docker exec airflow airflow tasks test goes_satellite_etl download_goes_data 2025-07-11
```

**Soluci√≥n**:
- Verificar conectividad a CITIC
- Comprobar URLs en `known_files`
- Instalar webdavclient3 en container

### Problema: Datos no aparecen en Druid
**Verificaci√≥n**:
```bash
# Ver logs de Kafka
docker-compose logs kafka

# Ver estado de ingesta en Druid
# http://localhost:8888 ‚Üí Ingestion ‚Üí Supervisors
```

### Problema: Superset no se conecta a Druid
**Soluci√≥n**:
```bash
# Verificar URI exacta
druid://router:8888/druid/v2/sql

# Verificar que Druid router est√© funcionando
curl http://localhost:8888/status/health
```

---

## üìä M√©tricas de Almacenamiento

**Archivos procesados**: 1 archivo NetCDF real
**Registros por archivo**: 30 registros (5 minutos de datos)
**Tama√±o archivo**: 187 KB
**Frecuencia datos**: Cada 10 segundos

**Estimaciones de crecimiento**:
- **1 d√≠a**: ~250 MB (144 archivos √ó 1.7 MB promedio)
- **1 semana**: ~1.75 GB 
- **1 mes**: ~7.5 GB

---

## üéØ Resultados Obtenidos

### ‚úÖ √âxitos Confirmados
1. **Descarga autom√°tica**: Archivo real de 187 KB descargado exitosamente
2. **Procesamiento completo**: 30 registros reales procesados con todas las columnas
3. **Pipeline funcional**: Datos fluyen de CITIC ‚Üí Druid ‚Üí Superset
4. **Datos verificados**: Query confirma datos reales (no sample_data.nc)
5. **Columnas correctas**: Todas las variables requeridas por el profesor presentes

### üìà Datos Procesados
- **Fuente**: OR_EXIS-L1b-SFXR_G18_s20231160000599_e20231160001294_c20231160001297.nc
- **Puntos temporales**: 30 registros
- **Per√≠odo**: 5 minutos de datos GOES reales
- **Variables**: irradiance_xrsa1, irradiance_xrsa2, irradiance_xrsb1, irradiance_xrsb2, primary_xrsb, dispersion_angle, integration_time

---

## üöÄ Pr√≥ximos Pasos

### Mejoras Inmediatas
1. **M√∫ltiples archivos**: Expandir la lista `known_files` con m√°s archivos de la misma fecha
2. **Fechas din√°micas**: Implementar descarga de m√∫ltiples d√≠as
3. **Exploraci√≥n autom√°tica**: Desarrollar listado din√°mico de archivos en CITIC

### Mejoras Avanzadas
1. **Scheduling inteligente**: DAG que ejecute autom√°ticamente para nuevos datos
2. **Alertas**: Notificaciones cuando fallen descargas
3. **M√©tricas avanzadas**: Dashboards de calidad de datos y performance del pipeline

### C√≥digo de Referencia para Expansi√≥n
```python
# Para agregar m√°s archivos del mismo d√≠a
known_files = [
    "1. GOES/Repositorio01/EXIS/SFXR/20230426/OR_EXIS-L1b-SFXR_G18_s20231160000599_e20231160001294_c20231160001297.nc",
    "1. GOES/Repositorio01/EXIS/SFXR/20230426/OR_EXIS-L1b-SFXR_G18_s20231160000299_e20231160000594_c20231160000598.nc",
    # Agregar m√°s archivos aqu√≠...
]

# Para fechas din√°micas
from datetime import datetime, timedelta
date_str = (datetime.now() - timedelta(days=365)).strftime('%Y%m%d')
```

---

## üìù Conclusiones

Este proyecto demuestra la implementaci√≥n exitosa de un pipeline de datos en tiempo real para datos cient√≠ficos reales. La integraci√≥n con el repositorio CITIC presenta un caso de uso real de acceso a datos gubernamentales/acad√©micos, mientras que la arquitectura de microservicios proporciona escalabilidad y mantenibilidad.

**Logros principales**:
- Pipeline funcional de extremo a extremo
- Integraci√≥n exitosa con fuente de datos externa
- Procesamiento de datos cient√≠ficos en tiempo real
- Documentaci√≥n completa para reproducibilidad

**Lecciones aprendidas**:
- La exploraci√≥n iterativa de APIs es crucial para fuentes de datos no documentadas
- Los scripts de prueba independientes aceleran el desarrollo
- La containerizaci√≥n facilita enormemente la reproducibilidad del entorno

---

## üë• Cr√©ditos

**Desarrollado para**: Proyecto Final del Curso
**Fuente de datos**: CITIC - Universidad de Costa Rica
**Datos utilizados**: GOES EXIS/SFXR - Extreme Ultraviolet and X-ray Irradiance Sensors
**Arquitectura base**: Open-Source Data Pipeline

**Fecha de implementaci√≥n**: Julio 2025
**Documentaci√≥n actualizada**: Julio 11, 2025
